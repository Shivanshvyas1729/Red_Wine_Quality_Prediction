{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bc2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798d9eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Projects\\\\Projects\\\\Machine learning\\\\Red_wine_project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8950475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceda93a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Projects\\\\Projects\\\\Machine learning\\\\Red_wine_project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c3cc7",
   "metadata": {},
   "source": [
    "1️⃣ What Is a dataclass?\n",
    "\n",
    "A dataclass is a Python feature that is used to store structured data (like config values) without writing boilerplate code.\n",
    "\n",
    "Instead of manually writing:\n",
    "\n",
    "__init__\n",
    "\n",
    "assignments\n",
    "\n",
    "repr methods\n",
    "\n",
    "Python does it for you automatically.\n",
    "\n",
    "2️⃣ Why Do We Use dataclass in ML Projects?\n",
    "Without dataclass ❌ (messy)\n",
    "class DataIngestionConfig:\n",
    "    def __init__(self):\n",
    "        self.root_dir = \"artifact/data_ingestion\"\n",
    "        self.source_URL = \"https://...\"\n",
    "        self.local_data_file = \"artifact/data_ingestion/data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b394185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "# @dataclass automatically creates:\n",
    "# - __init__ method\n",
    "# - readable __repr__\n",
    "# - comparison methods\n",
    "#\n",
    "# frozen=True makes the object IMMUTABLE\n",
    "# → once created, values cannot be changed\n",
    "# → ensures pipeline configuration stays consistent\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    # Directory where all data ingestion artifacts will be stored\n",
    "    # Example: artifact/data_ingestion/\n",
    "    root_dir: Path\n",
    "\n",
    "    # Public URL from where the dataset is downloaded\n",
    "    # Should be a direct downloadable link (GitHub raw, S3, etc.)\n",
    "    source_URL: str\n",
    "\n",
    "    # Local path where the downloaded ZIP file will be saved\n",
    "    # Example: artifact/data_ingestion/data.zip\n",
    "    local_data_file: Path\n",
    "\n",
    "    # Directory where the ZIP file will be extracted\n",
    "    # Final CSV files will be available here\n",
    "    unzip_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed4ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all constant variables defined in the constants module\n",
    "# These usually include fixed paths like:\n",
    "# CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "# Keeping them in one place avoids hardcoding paths across the project\n",
    "from Red_Wine_Prediction.constants import *\n",
    "\n",
    "# Import reusable utility functions\n",
    "# read_yaml       → reads YAML files (config.yaml, params.yaml, schema.yaml)\n",
    "# create_directories → safely creates required directories for artifacts\n",
    "# These utilities are shared across multiple pipeline stages\n",
    "from Red_Wine_Prediction.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707384fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager is responsible for:\n",
    "    - Reading all YAML configuration files (config, params, schema)\n",
    "    - Creating the main artifacts directory\n",
    "    - Providing stage-specific configuration objects (dataclasses)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Path = CONFIG_FILE_PATH,\n",
    "        params_filepath: Path = PARAMS_FILE_PATH,\n",
    "        schema_filepath: Path = SCHEMA_FILE_PATH,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (Path): Path to config.yaml (pipeline paths & stages)\n",
    "            params_filepath (Path): Path to params.yaml (model hyperparameters)\n",
    "            schema_filepath (Path): Path to schema.yaml (data validation rules)\n",
    "        \"\"\"\n",
    "\n",
    "        # Load YAML files once and store them in memory\n",
    "        # This avoids reading files repeatedly in each pipeline stage\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        # Create the root artifacts directory\n",
    "        # All pipeline outputs (data, models, reports) will live inside this folder\n",
    "        create_directories([Path(self.config.artifacts_root)])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Creates and returns a DataIngestionConfig object.\n",
    "\n",
    "        This method:\n",
    "        - Extracts the data_ingestion section from config.yaml\n",
    "        - Creates required directories for the ingestion stage\n",
    "        - Converts raw YAML values into strongly-typed Path objects\n",
    "\n",
    "        Returns:\n",
    "            DataIngestionConfig: Immutable config object for data ingestion stage\n",
    "        \"\"\"\n",
    "\n",
    "        # Access the data_ingestion section from config.yaml\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        # Create the data ingestion root directory\n",
    "        create_directories([Path(config.root_dir)])\n",
    "\n",
    "        # Convert YAML config values into a dataclass\n",
    "        # Using Path ensures OS-independent file handling\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=Path(config.local_data_file),\n",
    "            unzip_dir=Path(config.unzip_dir),\n",
    "        )\n",
    "\n",
    "        # Return the prepared configuration to the DataIngestion component\n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a05604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Provides access to system-specific parameters and functions\n",
    "# Used mainly for error handling or exiting the program safely\n",
    "\n",
    "import urllib.request as request\n",
    "# Used to download files from a URL\n",
    "# In this project, it downloads the dataset ZIP file during data ingestion\n",
    "\n",
    "import zipfile\n",
    "# Used to extract ZIP files\n",
    "# After downloading the dataset, this module unzips the contents\n",
    "\n",
    "from Red_Wine_Prediction import logger\n",
    "# Project-wide logger instance\n",
    "# Used for consistent logging across all pipeline stages\n",
    "\n",
    "from Red_Wine_Prediction.utils.common import get_size\n",
    "# Utility function to get file size in KB\n",
    "# Used to log dataset size after downloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f1b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    DataIngestion handles:\n",
    "    - Downloading the raw dataset from a remote source\n",
    "    - Extracting the downloaded ZIP file into the artifact directory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DataIngestionConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DataIngestion class.\n",
    "\n",
    "        Args:\n",
    "            config (DataIngestionConfig): Configuration object containing\n",
    "                                          paths and source URL for ingestion\n",
    "        \"\"\"\n",
    "        # Store the ingestion configuration\n",
    "        self.config = config\n",
    "\n",
    "    def download_file(self) -> str:\n",
    "        \"\"\"\n",
    "        Downloads the dataset ZIP file from the source URL.\n",
    "\n",
    "        - If the file already exists locally, it skips downloading\n",
    "        - Logs file size if already present\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the dataset ZIP file already exists\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "\n",
    "            # Download the file from the given URL\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url=self.config.source_URL,\n",
    "                filename=self.config.local_data_file\n",
    "            )\n",
    "\n",
    "            # Log download success and HTTP response headers\n",
    "            logger.info(f\"{filename} downloaded! with following info: \\n{headers}\")\n",
    "\n",
    "        else:\n",
    "            # If file already exists, log its size\n",
    "            logger.info(\n",
    "                f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\"\n",
    "            )\n",
    "\n",
    "    def extract_zip_file(self) -> None:\n",
    "        \"\"\"\n",
    "        Extracts the downloaded ZIP file into the specified directory.\n",
    "\n",
    "        - Creates the unzip directory if it does not exist\n",
    "        - Extracts all files from the ZIP\n",
    "        \"\"\"\n",
    "\n",
    "        # Path where the ZIP contents will be extracted\n",
    "        unzip_path = self.config.unzip_dir\n",
    "\n",
    "        # Ensure the extraction directory exists\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "\n",
    "        # Open and extract the ZIP file\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "\n",
    "        # Log successful extraction\n",
    "        logger.info(f\"File extracted to {unzip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64285928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-02 21:34:18,039: INFO: common: YAML file loaded successfully: config\\config.yaml]\n",
      "[2026-01-02 21:34:18,040: INFO: common: YAML file loaded successfully: params.yaml]\n",
      "[2026-01-02 21:34:18,042: INFO: common: YAML file loaded successfully: schema.yaml]\n",
      "[2026-01-02 21:34:18,043: INFO: common: Directory created at: artifacts]\n",
      "[2026-01-02 21:34:18,044: INFO: common: Directory created at: artifacts\\data_ingestion]\n",
      "[2026-01-02 21:34:19,947: INFO: 1496518022: artifacts\\data_ingestion\\data.zip downloaded! with following info: \n",
      "Connection: close\n",
      "Content-Length: 23329\n",
      "Cache-Control: max-age=300\n",
      "Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox\n",
      "Content-Type: application/zip\n",
      "ETag: \"c69888a4ae59bc5a893392785a938ccd4937981c06ba8a9d6a21aa52b4ab5b6e\"\n",
      "Strict-Transport-Security: max-age=31536000\n",
      "X-Content-Type-Options: nosniff\n",
      "X-Frame-Options: deny\n",
      "X-XSS-Protection: 1; mode=block\n",
      "X-GitHub-Request-Id: CEFA:2B10AF:1505625:26E50AF:6957EC84\n",
      "Accept-Ranges: bytes\n",
      "Date: Fri, 02 Jan 2026 16:04:21 GMT\n",
      "Via: 1.1 varnish\n",
      "X-Served-By: cache-del21739-DEL\n",
      "X-Cache: MISS\n",
      "X-Cache-Hits: 0\n",
      "X-Timer: S1767369861.846626,VS0,VE271\n",
      "Vary: Authorization,Accept-Encoding\n",
      "Access-Control-Allow-Origin: *\n",
      "Cross-Origin-Resource-Policy: cross-origin\n",
      "X-Fastly-Request-ID: 81253495eb4c0373999dd276ad11a6e9a44531ef\n",
      "Expires: Fri, 02 Jan 2026 16:09:21 GMT\n",
      "Source-Age: 0\n",
      "\n",
      "]\n",
      "[2026-01-02 21:34:19,971: INFO: 1496518022: File extracted to artifacts\\data_ingestion]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize ConfigurationManager\n",
    "    # This loads all YAML files (config.yaml, params.yaml, schema.yaml)\n",
    "    # and prepares the root artifact directory\n",
    "    config = ConfigurationManager()\n",
    "\n",
    "    # Get the configuration specific to the Data Ingestion stage\n",
    "    # This returns a DataIngestionConfig dataclass\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "\n",
    "    # Initialize the DataIngestion component with its configuration\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "\n",
    "    # Download the dataset ZIP file (if not already present)\n",
    "    data_ingestion.download_file()\n",
    "\n",
    "    # Extract the downloaded ZIP file into the artifact directory\n",
    "    data_ingestion.extract_zip_file()\n",
    "\n",
    "except Exception as e:\n",
    "    # If any error occurs during the data ingestion stage,\n",
    "    # re-raise the exception so it can be logged and handled by main.py\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419a3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Miniconda3\\envs\\Red_Wine_Predication\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96796b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Red_Wine_Predication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
